# Hadoop é›†æˆæµ‹è¯•æŒ‡å—

## ğŸ“‹ æµ‹è¯•å‰å‡†å¤‡

### 1. ç¡®ä¿æ‰€æœ‰æœåŠ¡è¿è¡Œ

```bash
# æ£€æŸ¥ Docker å®¹å™¨çŠ¶æ€
docker ps

# åº”è¯¥çœ‹åˆ°ä»¥ä¸‹å®¹å™¨è¿è¡Œä¸­ï¼š
# - hadoop-namenode
# - hadoop-datanode
# - hadoop-resourcemanager
# - hadoop-nodemanager (å¯é€‰ï¼Œä½†å»ºè®®è¿è¡Œ)
# - kg-redis
# - kg-neo4j
# - kg-mysql
# - kg-backend (å¦‚æœä½¿ç”¨ Docker)
# - kg-celery-worker (å¦‚æœä½¿ç”¨ Docker)
```

### 2. ç¡®ä¿åç«¯æœåŠ¡è¿è¡Œ

å¦‚æœä½¿ç”¨æœ¬åœ°è¿è¡Œï¼ˆé Dockerï¼‰ï¼š
```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•
python run.py
# æˆ–
uvicorn backend.app:app --host 0.0.0.0 --port 5001
```

## ğŸ§ª æµ‹è¯•æ­¥éª¤

### æ­¥éª¤ 1: è¿è¡ŒåŸºç¡€é›†æˆæµ‹è¯•

```bash
python scripts/test_hadoop_integration.py
```

è¿™ä¸ªè„šæœ¬ä¼šæµ‹è¯•ï¼š
- âœ… HDFS åŸºæœ¬æ“ä½œï¼ˆåˆ›å»ºç›®å½•ã€ä¸Šä¼ æ–‡ä»¶ã€è¯»å–æ–‡ä»¶ï¼‰
- âœ… MapReduce è„šæœ¬ä¸Šä¼ åŠŸèƒ½
- âœ… Hadoop æœåŠ¡åˆå§‹åŒ–
- âœ… Celery é›†æˆæ£€æŸ¥

### æ­¥éª¤ 2: è¿è¡Œå®Œæ•´ç«¯åˆ°ç«¯æµ‹è¯•

#### æ–¹å¼ A: ä½¿ç”¨æµ‹è¯•è„šæœ¬ï¼ˆæ¨èï¼‰

1. **å‡†å¤‡æµ‹è¯• PDF æ–‡ä»¶**
   - å°†æµ‹è¯• PDF æ–‡ä»¶æ”¾åœ¨ `data/raw/` ç›®å½•
   - æˆ–ä¿®æ”¹ `tests/test_batch_build.py` ä¸­çš„ `PDF_FILES` è·¯å¾„

2. **è¿è¡Œæµ‹è¯•è„šæœ¬**
   ```bash
   python tests/test_batch_build.py
   ```

   è¿™ä¸ªè„šæœ¬ä¼šï¼š
   - æ‰¹é‡ä¸Šä¼  PDF æ–‡ä»¶åˆ°åç«¯å’Œ HDFS
   - è§¦å‘ Hadoop MapReduce å¤„ç†ï¼ˆPDFæå– â†’ æ–‡æœ¬æ¸…æ´— â†’ æ–‡æœ¬åˆ†å—ï¼‰
   - è§¦å‘ Celery ä»»åŠ¡æ„å»ºçŸ¥è¯†å›¾è°±
   - è½®è¯¢ä»»åŠ¡çŠ¶æ€ç›´åˆ°å®Œæˆ

#### æ–¹å¼ B: ä½¿ç”¨ API æ‰‹åŠ¨æµ‹è¯•

1. **æ‰¹é‡ä¸Šä¼ æ–‡ä»¶**
   ```bash
   curl -X POST "http://localhost:5001/api/hadoop/upload/batch" \
     -F "files=@/path/to/test1.pdf" \
     -F "files=@/path/to/test2.pdf"
   ```

   æˆ–ä½¿ç”¨ Pythonï¼š
   ```python
   import requests
   
   files = [
       ('files', ('test1.pdf', open('test1.pdf', 'rb'), 'application/pdf')),
       ('files', ('test2.pdf', open('test2.pdf', 'rb'), 'application/pdf')),
   ]
   
   response = requests.post(
       'http://localhost:5001/api/hadoop/upload/batch',
       files=files
   )
   print(response.json())
   ```

2. **è§¦å‘æ‰¹é‡æ„å»º**
   ```bash
   curl -X POST "http://localhost:5001/api/hadoop/build/batch" \
     -H "Content-Type: application/json" \
     -d '{
       "file_ids": ["file-id-1", "file-id-2"],
       "use_hadoop": true
     }'
   ```

3. **æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€**
   ```bash
   curl "http://localhost:5001/api/hadoop/status/{task_id}"
   ```

### æ­¥éª¤ 3: éªŒè¯å¤„ç†ç»“æœ

#### æ£€æŸ¥ HDFS ä¸­çš„å¤„ç†ç»“æœ

```bash
# æŸ¥çœ‹ PDF æå–ç»“æœ
docker exec hadoop-namenode hadoop fs -ls /knowledge_graph/processed/pdf_extract

# æŸ¥çœ‹æ–‡æœ¬æ¸…æ´—ç»“æœ
docker exec hadoop-namenode hadoop fs -ls /knowledge_graph/processed/text_clean

# æŸ¥çœ‹æ–‡æœ¬åˆ†å—ç»“æœ
docker exec hadoop-namenode hadoop fs -ls /knowledge_graph/processed/text_chunk

# æŸ¥çœ‹æŸä¸ªåˆ†å—çš„å†…å®¹
docker exec hadoop-namenode hadoop fs -cat /knowledge_graph/processed/text_chunk/part-00000 | head -20
```

#### æ£€æŸ¥çŸ¥è¯†å›¾è°±æ„å»ºç»“æœ

```bash
# é€šè¿‡ API æŸ¥è¯¢å›¾è°±åˆ—è¡¨
curl "http://localhost:5001/api/kg/list"

# æŸ¥è¯¢ç‰¹å®šå›¾è°±
curl "http://localhost:5001/api/kg/{graph_id}"

# æŸ¥çœ‹å›¾è°±å¯è§†åŒ–æ•°æ®
curl "http://localhost:5001/api/kg/{graph_id}/visualize"
```

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: HDFS è¿æ¥å¤±è´¥

**ç—‡çŠ¶ï¼š** `Connection refused` æˆ– `NameNode not found`

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ£€æŸ¥ NameNode å®¹å™¨çŠ¶æ€
docker ps --filter "name=hadoop-namenode"

# æ£€æŸ¥ NameNode æ—¥å¿—
docker logs hadoop-namenode --tail 50

# éªŒè¯ç«¯å£é…ç½®
docker exec hadoop-namenode cat /etc/hadoop/core-site.xml | grep fs.defaultFS
# åº”è¯¥æ˜¾ç¤º: hdfs://hadoop-namenode:8020
```

### é—®é¢˜ 2: MapReduce ä»»åŠ¡å¤±è´¥

**ç—‡çŠ¶ï¼š** ä»»åŠ¡æäº¤åç«‹å³å¤±è´¥æˆ–è¶…æ—¶

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ£€æŸ¥ NodeManager æ˜¯å¦è¿è¡Œ
docker ps --filter "name=hadoop-nodemanager"

# æ£€æŸ¥ ResourceManager æ—¥å¿—
docker logs hadoop-resourcemanager --tail 50

# æ£€æŸ¥ NodeManager æ—¥å¿—
docker logs hadoop-nodemanager --tail 50

# éªŒè¯ Python å’Œä¾èµ–
docker exec hadoop-namenode python3 --version
docker exec hadoop-namenode python3 -c "import pdfplumber; print('OK')"
```

### é—®é¢˜ 3: PDF æå–å¤±è´¥

**ç—‡çŠ¶ï¼š** PDF æå–ä»»åŠ¡è¿”å›é”™è¯¯

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ£€æŸ¥ pdfplumber æ˜¯å¦å®‰è£…
docker exec hadoop-namenode python3 -c "import pdfplumber"

# å¦‚æœæœªå®‰è£…ï¼Œå®‰è£…å®ƒ
docker exec hadoop-namenode pip3 install pdfplumber

# æˆ–ä½¿ç”¨å›½å†…é•œåƒæºï¼ˆæ›´å¿«ï¼‰
docker exec hadoop-namenode pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pdfplumber
```

### é—®é¢˜ 4: Celery ä»»åŠ¡æœªæ‰§è¡Œ

**ç—‡çŠ¶ï¼š** Hadoop å¤„ç†å®Œæˆä½† Celery ä»»åŠ¡æœªå¯åŠ¨

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ£€æŸ¥ Redis è¿æ¥
docker exec kg-redis redis-cli ping

# æ£€æŸ¥ Celery Worker æ—¥å¿—
docker logs kg-celery-worker --tail 50

# æ£€æŸ¥ Celery Worker æ˜¯å¦è¿è¡Œ
docker ps --filter "name=kg-celery-worker"
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### æŸ¥çœ‹ Hadoop Web UI

- **NameNode Web UI**: http://localhost:9870
- **ResourceManager Web UI**: http://localhost:8088
- **DataNode Web UI**: http://localhost:9864

### æŸ¥çœ‹åº”ç”¨æ—¥å¿—

```bash
# åç«¯æ—¥å¿—
tail -f logs/app.log

# Celery Worker æ—¥å¿—
docker logs -f kg-celery-worker

# Hadoop å®¹å™¨æ—¥å¿—
docker logs -f hadoop-namenode
docker logs -f hadoop-resourcemanager
```

## âœ… æˆåŠŸæ ‡å‡†

å®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯•æˆåŠŸåº”è¯¥æ»¡è¶³ï¼š

1. âœ… æ–‡ä»¶æˆåŠŸä¸Šä¼ åˆ° HDFS
2. âœ… PDF æå–ä»»åŠ¡æˆåŠŸå®Œæˆ
3. âœ… æ–‡æœ¬æ¸…æ´—ä»»åŠ¡æˆåŠŸå®Œæˆ
4. âœ… æ–‡æœ¬åˆ†å—ä»»åŠ¡æˆåŠŸå®Œæˆ
5. âœ… Celery ä»»åŠ¡æˆåŠŸä¸‹è½½æ–‡æœ¬å—
6. âœ… çŸ¥è¯†å›¾è°±æˆåŠŸæ„å»ºï¼ˆæœ‰å®ä½“å’Œå…³ç³»ï¼‰
7. âœ… å¯ä»¥é€šè¿‡ API æŸ¥è¯¢åˆ°æ„å»ºçš„å›¾è°±

## ğŸš€ ä¸‹ä¸€æ­¥

æµ‹è¯•é€šè¿‡åï¼Œä½ å¯ä»¥ï¼š

1. **ä¼˜åŒ–æ€§èƒ½**
   - è°ƒæ•´ MapReduce ä»»åŠ¡å‚æ•°
   - ä¼˜åŒ–æ–‡æœ¬åˆ†å—å¤§å°
   - è°ƒæ•´ Celery Worker å¹¶å‘æ•°

2. **æ‰©å±•åŠŸèƒ½**
   - æ·»åŠ æ›´å¤šæ–‡æœ¬å¤„ç†æ­¥éª¤
   - ä¼˜åŒ–åŒ»å­¦æ–‡æœ¬æ¸…æ´—è§„åˆ™
   - æ·»åŠ ä»»åŠ¡ç›‘æ§å’Œå‘Šè­¦

3. **ç”Ÿäº§éƒ¨ç½²**
   - é…ç½®æŒä¹…åŒ–å­˜å‚¨
   - è®¾ç½®èµ„æºé™åˆ¶
   - é…ç½®æ—¥å¿—è½®è½¬
   - æ·»åŠ å¥åº·æ£€æŸ¥

---

**æœ€åæ›´æ–°**: 2026å¹´1æœˆ  
**çŠ¶æ€**: æµ‹è¯•å°±ç»ª

