"""
PDF åŒ»ç–—æ–‡æ¡£å¯¼å…¥è„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
åœºæ™¯ï¼šåŒ»å­¦è®ºæ–‡ / ç–¾ç—…æŒ‡å— / å¥åº·æŒ‡å—ç­‰ PDF

æµç¨‹ï¼š
1. ä» PDF æå–åŸå§‹æ–‡æœ¬
2. é’ˆå¯¹åŒ»å­¦åœºæ™¯è¿›è¡Œæ–‡æœ¬æ¸…æ´—ï¼š
   - åˆ‡æ‰å‚è€ƒæ–‡çŒ®/è‡´è°¢ä¹‹åçš„å†…å®¹
   - å»æ‰å›¾è¡¨æ ‡é¢˜ã€è¡¨æ ¼ã€é¡µçœ‰é¡µè„šç­‰å™ªéŸ³
   - åˆ é™¤æ–‡çŒ®å¼•ç”¨æ ‡è®°ã€URLã€é‚®ç®±ç­‰
3. å°†æ¸…æ´—åçš„çº¯æ–‡æœ¬æŒ‰æ®µåˆ‡åˆ†
4. äº¤ç»™ KnowledgeGraphBuilder.process_text æ„å»ºçŸ¥è¯†å›¾è°±
"""

import sys
import time
import re
from pathlib import Path
from typing import Dict, Any, List

# æŠŠé¡¹ç›®æ ¹ç›®å½•åŠ å…¥ Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import logging
import config
from db.neo4j_client import Neo4jClient
from llm.client import LLMClient, EmbeddingClient
from kg.builder import KnowledgeGraphBuilder

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=config.LOG_LEVEL,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def extract_text_from_pdf(pdf_path: str) -> str:
    """
    ä» PDF æ–‡ä»¶ä¸­æå–å…¨éƒ¨åŸå§‹æ–‡æœ¬ï¼ˆæœªæ¸…æ´—ï¼‰

    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„

    Returns:
        æå–åˆ°çš„å…¨éƒ¨æ–‡æœ¬ï¼ˆå­—ç¬¦ä¸²ï¼‰
    """
    try:
        import pdfplumber
    except ImportError:
        logger.error(
            "æœªå®‰è£… pdfplumber åº“ï¼Œæ— æ³•è§£æ PDFã€‚\n"
            "è¯·å…ˆè¿è¡Œ: pip install pdfplumber"
        )
        raise

    pdf_file = Path(pdf_path)
    if not pdf_file.exists():
        raise FileNotFoundError(f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")

    logger.info(f"å¼€å§‹ä» PDF æå–æ–‡æœ¬: {pdf_file}")

    texts: List[str] = []
    with pdfplumber.open(pdf_file) as pdf:
        logger.info(f"PDF å…± {len(pdf.pages)} é¡µ")
        for i, page in enumerate(pdf.pages):
            page_text = page.extract_text() or ""
            # ç²—æš´å»æ‰å¤šä½™ç©ºç™½
            page_text = page_text.replace("\u00a0", " ")
            logger.info(f"ç¬¬ {i + 1}/{len(pdf.pages)} é¡µï¼Œæå– {len(page_text)} å­—ç¬¦")
            texts.append(page_text)

    full_text = "\n\n".join(texts)
    logger.info(f"PDF æ–‡æœ¬æå–å®Œæˆï¼Œæ€»é•¿åº¦ {len(full_text)} å­—ç¬¦")
    return full_text


def clean_medical_text(raw_text: str) -> str:
    """
    é’ˆå¯¹åŒ»å­¦è®ºæ–‡ / æŒ‡å—çš„æ–‡æœ¬è¿›è¡Œæ¸…æ´—ï¼Œåªä¿ç•™ç›¸å¯¹æœ‰ç”¨çš„åŒ»å­¦æ­£æ–‡å†…å®¹

    ä¸»è¦æ“ä½œï¼š
    1. æˆªæ–­å‚è€ƒæ–‡çŒ®/è‡´è°¢ä¹‹åçš„å†…å®¹
    2. è¿‡æ»¤æ‰å›¾è¡¨æ ‡é¢˜ã€é¡µçœ‰é¡µè„šã€çº¯æ•°å­—/ç¬¦å·è¡Œ
    3. åˆ é™¤è¡Œå†…çš„å‚è€ƒæ–‡çŒ®æ ‡è®°ã€URLã€é‚®ç®±ç­‰

    Args:
        raw_text: åŸå§‹ PDF æ–‡æœ¬

    Returns:
        æ¸…æ´—åçš„æ–‡æœ¬
    """
    if not raw_text:
        return ""

    # ç»Ÿä¸€æ¢è¡Œç¬¦
    text = raw_text.replace("\r\n", "\n").replace("\r", "\n")

    # 1) æŒ‰å‚è€ƒæ–‡çŒ®/è‡´è°¢æˆªæ–­
    cutoff_patterns = [
        r"^\s*å‚è€ƒæ–‡çŒ®\s*$",
        r"^\s*å‚è€ƒèµ„æ–™\s*$",
        r"^\s*è‡´è°¢\s*$",
        r"^\s*Acknowledg?ement[s]?\s*$",
        r"^\s*References\s*$",
        r"^\s*BIBLIOGRAPHY\s*$",
        r"^\s*Bibliography\s*$",
    ]
    cutoff_regex = re.compile("|".join(cutoff_patterns), re.IGNORECASE)

    lines = text.split("\n")
    filtered_lines: List[str] = []

    medical_keywords = [
        "ç‚", "ç™Œ", "ç»¼åˆå¾", "ç»¼åˆç—‡", "ç—‡", "ç–¾ç—…", "ç—…å› ", "ç—…ç¨‹", "ç—…ç†", "ç—…å˜",
        "è¯Šæ–­", "æ²»ç–—", "ç”¨è¯", "è¯ç‰©", "æ–¹æ¡ˆ", "ç–—æ³•", "å¹²é¢„", "é¢„å", "é¢„é˜²",
        "é£é™©", "å±é™©å› ç´ ", "å¹¶å‘ç—‡", "æ„ŸæŸ“", "å‡ºè¡€", "åæ­»",
        "æ‚£è€…", "ç—…äºº", "ä¸´åºŠ", "æŒ‡å—", "æ¨è", "éšè®¿", "å¤å‘",
        "èƒ°è…º", "èƒ°è…ºç‚", "èƒ°è…ºç™Œ", "è‚", "è‚¾", "å¿ƒåŠŸèƒ½",
        # è‹±æ–‡
        "pancreatitis", "pancreas", "acute", "chronic",
        "disease", "syndrome", "disorder",
        "treatment", "therapy", "management",
        "diagnosis", "diagnostic",
        "clinical", "patient", "patients",
        "risk", "factor", "complication", "outcome", "prognosis"
    ]

    def looks_like_figure_or_table(line: str) -> bool:
        line_strip = line.strip()
        # å›¾è¡¨æ ‡é¢˜
        if re.match(r"^(å›¾|è¡¨)\s*\d+", line_strip):
            return True
        if re.match(r"^(Figure|Fig\.?|Table|TAB\.)\s*\d+", line_strip, re.IGNORECASE):
            return True
        return False

    def is_mostly_numeric_or_garbage(line: str) -> bool:
        # å¤ªçŸ­çš„è¡Œå¦ä¸€å¥—é€»è¾‘å¤„ç†ï¼Œè¿™é‡Œåªé’ˆå¯¹æœ‰äº›é•¿åº¦ä½†å†…å®¹æ˜¯æ•°å­—/ç¬¦å·çš„
        if len(line) < 6:
            return True
        chars = [c for c in line if not c.isspace()]
        if not chars:
            return True
        digits = sum(c.isdigit() for c in chars)
        punct = sum(c in ".,;:[]()%+-=<>/\\|~" for c in chars)
        ratio = (digits + punct) / max(len(chars), 1)
        return ratio > 0.6

    def contains_medical_keyword(line: str) -> bool:
        lower = line.lower()
        return any(k in line or k in lower for k in medical_keywords)

    # 2) é€è¡Œå¤„ç† + æˆªæ–­å‚è€ƒæ–‡çŒ®
    for line in lines:
        # æˆªæ–­é€»è¾‘ï¼šé‡åˆ°å‚è€ƒæ–‡çŒ® / è‡´è°¢ç­‰ç›´æ¥ç»“æŸ
        if cutoff_regex.match(line):
            logger.info(f"æ£€æµ‹åˆ°å‚è€ƒæ–‡çŒ®/è‡´è°¢æ ‡è®°è¡Œ: {line.strip()}ï¼Œåç»­å†…å®¹å°†è¢«å¿½ç•¥")
            break

        line_strip = line.strip()
        if not line_strip:
            continue

        # é¡µçœ‰é¡µè„šç²—ç•¥è¿‡æ»¤ï¼šå¸¦ Page / é¡µ / æœŸåˆŠå·ç­‰ä¸”å‡ ä¹æ²¡åŒ»å­¦è¯
        if re.search(r"Page\s+\d+\s+of\s+\d+", line_strip, re.IGNORECASE):
            continue
        if re.search(r"ç¬¬\s*\d+\s*é¡µ", line_strip):
            continue

        # å›¾è¡¨æ ‡é¢˜
        if looks_like_figure_or_table(line_strip):
            continue

        # çº¯æ•°å­—/ç¬¦å·
        if is_mostly_numeric_or_garbage(line_strip):
            continue

        # å¾ˆçŸ­ä¸”ä¸å«åŒ»å­¦å…³é”®è¯ â†’ ä¸¢æ‰ï¼ˆå¤§æ¦‚ç‡æ˜¯æ ç›®æ ‡é¢˜/åƒåœ¾æ’ç‰ˆï¼‰
        if len(line_strip) < 15 and not contains_medical_keyword(line_strip):
            continue

        # åŠ å…¥åç»­æ¸…æ´—æµç¨‹
        filtered_lines.append(line_strip)

    # 3) è¡Œå†…è½»é‡æ¸…æ´—
    cleaned_lines: List[str] = []
    for line in filtered_lines:
        # åˆ æ‰ [1] [2-5] è¿™ç±»å¼•ç”¨æ ‡è®°
        line = re.sub(r"\[[0-9,\-\s]+\]", "", line)

        # åˆ é™¤ç®€å•æ‹¬å·å†…æ–‡çŒ®å¼•ç”¨ï¼Œä¾‹å¦‚ (Smith 2020), (Wang et al., 2019)
        line = re.sub(r"\([A-Z][A-Za-z].{0,40}?\d{4}\)", "", line)

        # å»æ‰ URL
        line = re.sub(r"http[s]?://\S+", "", line)

        # å»æ‰é‚®ç®±
        line = re.sub(r"\b[\w\.-]+@[\w\.-]+\.\w+\b", "", line)

        # å¤šç©ºæ ¼å‹ç¼©
        line = re.sub(r"\s{2,}", " ", line).strip()
        if line:
            cleaned_lines.append(line)

    cleaned_text = "\n".join(cleaned_lines)
    logger.info(f"æ¸…æ´—åæ–‡æœ¬é•¿åº¦: {len(cleaned_text)} å­—ç¬¦ï¼ˆåŸå§‹ {len(raw_text)}ï¼‰")
    return cleaned_text


def import_pdf_to_kg(
    pdf_path: str,
    kg_builder: KnowledgeGraphBuilder,
    chunk_size: int = 1000,
    sleep_sec: float = 1.0
) -> Dict[str, Any]:
    """
    ä» PDF å¯¼å…¥çŸ¥è¯†åˆ°å›¾è°±ï¼ˆå¸¦åŒ»ç–—åœºæ™¯æ¸…æ´—ï¼‰

    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        kg_builder: çŸ¥è¯†å›¾è°±æ„å»ºå™¨å®ä¾‹
        chunk_size: æ¯ä¸ªåˆ†æ®µçš„å­—ç¬¦æ•°
        sleep_sec: æ¯æ®µä¹‹é—´çš„ä¼‘çœ æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    # 1. æå– PDF åŸå§‹æ–‡æœ¬
    raw_text = extract_text_from_pdf(pdf_path)

    # 2. åŒ»å­¦åœºæ™¯æ¸…æ´—
    clean_text = clean_medical_text(raw_text)

    if not clean_text.strip():
        logger.warning("æ¸…æ´—åæ–‡æœ¬å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡å¯¼å…¥")
        return {
            "total_chunks": 0,
            "processed_chunks": 0,
            "entities_created": 0,
            "relations_created": 0,
            "errors": 0,
        }

    # 3. åˆ†æ®µ
    chunks = [
        clean_text[i:i + chunk_size]
        for i in range(0, len(clean_text), chunk_size)
    ]
    logger.info(f"æ¸…æ´—æ–‡æœ¬åˆ†ä¸º {len(chunks)} æ®µï¼ˆchunk_size={chunk_size}ï¼‰")

    stats = {
        "total_chunks": len(chunks),
        "processed_chunks": 0,
        "entities_created": 0,
        "relations_created": 0,
        "errors": 0,
    }

    # 4. å¯¹æ¯ä¸ªæ–‡æœ¬æ®µè°ƒç”¨ KnowledgeGraphBuilder
    for idx, chunk in enumerate(chunks, start=1):
        logger.info(f"å¤„ç†ç¬¬ {idx}/{len(chunks)} æ®µ...")
        try:
            result = kg_builder.process_text(chunk)

            stats["processed_chunks"] += 1

            # æ ¹æ®ä½ çš„å®é™…è¿”å›ç»“æ„åšå…œåº•
            entities = (
                result.get("entities_created")
                or result.get("entity_count")
                or 0
            )
            relations = (
                result.get("relations_created")
                or result.get("relation_count")
                or 0
            )

            stats["entities_created"] += entities
            stats["relations_created"] += relations

            logger.info(
                f"âœ“ ç¬¬ {idx} æ®µå¤„ç†å®Œæˆ: "
                f"æ–°å¢å®ä½“={entities}, æ–°å¢å…³ç³»={relations}"
            )

            if sleep_sec > 0:
                time.sleep(sleep_sec)

        except Exception as e:
            logger.error(f"å¤„ç†ç¬¬ {idx} æ®µå¤±è´¥: {e}")
            stats["errors"] += 1

    logger.info(f"PDF å¯¼å…¥å®Œæˆ: {stats}")
    return stats


def main():
    print("=" * 70)
    print("ğŸ“„ PDF åŒ»å­¦æ–‡çŒ®å¯¼å…¥å·¥å…·ï¼ˆå¸¦æ¸…æ´—ï¼‰")
    print("=" * 70)

    # 1. è·å– PDF è·¯å¾„
    if len(sys.argv) >= 2:
        pdf_path = sys.argv[1]
    else:
        pdf_path = input("è¯·è¾“å…¥ PDF æ–‡ä»¶è·¯å¾„: ").strip()

    if not pdf_path:
        print("âŒ æœªæä¾› PDF è·¯å¾„ï¼Œé€€å‡º")
        return

    # 2. åˆ›å»ºå®¢æˆ·ç«¯
    logger.info("æ­£åœ¨åˆå§‹åŒ–æœåŠ¡...")

    neo4j_client = None
    llm_client = None
    embedding_client = None

    try:
        neo4j_client = Neo4jClient()
        if not neo4j_client.verify_connection():
            logger.error("âŒ Neo4j è¿æ¥å¤±è´¥")
            return

        llm_client = LLMClient()
        if not llm_client.verify_connection():
            logger.error("âŒ LLM è¿æ¥å¤±è´¥")
            return

        embedding_client = EmbeddingClient()

        # 3. åˆ›å»ºçŸ¥è¯†å›¾è°±æ„å»ºå™¨
        kg_builder = KnowledgeGraphBuilder(
            neo4j_client=neo4j_client,
            llm_client=llm_client,
        )

        # 4. å¯¼å…¥ PDF
        stats = import_pdf_to_kg(pdf_path, kg_builder)

        print("\nâœ… PDF å¯¼å…¥å®Œæˆ!")
        print(f"   æ–‡æœ¬åˆ†æ®µ: {stats['processed_chunks']}/{stats['total_chunks']}")
        print(f"   æ–°å¢å®ä½“: {stats['entities_created']}")
        print(f"   æ–°å¢å…³ç³»: {stats['relations_created']}")
        print(f"   å‡ºé”™æ®µæ•°: {stats['errors']}")

    except KeyboardInterrupt:
        print("\nâ¹ï¸  å¯¼å…¥å·²å–æ¶ˆ")
    except Exception as e:
        logger.error(f"å¯¼å…¥è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # 5. å…³é—­è¿æ¥
        if neo4j_client:
            neo4j_client.close()
        if llm_client:
            # ä½ å·²ç»ç»™ DeepSeekClient å®ç°äº† close()
            llm_client.close()
        logger.info("è¿æ¥å·²å…³é—­")

        print("\n" + "=" * 70)


if __name__ == "__main__":
    main()
