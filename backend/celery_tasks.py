"""
Celery ä»»åŠ¡å®šä¹‰
ç”¨äºå¼‚æ­¥å¤„ç†çŸ¥è¯†å›¾è°±æ„å»ºä»»åŠ¡
"""

import os
import sys
import logging
from typing import Dict, Any, Optional
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from backend.celery_app import celery_app
except ImportError:
    # å¦‚æœåœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ,å¯èƒ½éœ€è¦è°ƒæ•´å¯¼å…¥è·¯å¾?
    sys.path.insert(0, str(project_root.parent) if project_root.parent.exists() else str(project_root))
    from backend.celery_app import celery_app

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    from hadoop.utils.hdfs_client import HDFSClient
except ImportError:
    HDFSClient = None
    logger.warning("HDFSClient å¯¼å…¥å¤±è´¥,HDFS åŠŸèƒ½å°†ä¸å¯ç”¨")

# å…¨å±€å˜é‡(å»¶è¿Ÿåˆå§‹åŒ?
neo4j_client = None
llm_client = None
embedding_client = None
kg_builder = None


def get_kg_builder():
    """è·å–çŸ¥è¯†å›¾è°±æ„å»ºå™¨å®ä¾?å•ä¾‹æ¨¡å¼)"""
    global neo4j_client, llm_client, embedding_client, kg_builder
    
    if kg_builder is None:
        try:
            from db.neo4j_client import Neo4jClient
            from llm.client import LLMClient, EmbeddingClient
            from kg.builder import KnowledgeGraphBuilder
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            neo4j_client = Neo4jClient()
            llm_client = LLMClient()
            embedding_client = EmbeddingClient()
            
            # åˆ›å»ºçŸ¥è¯†å›¾è°±æ„å»ºå™?
            kg_builder = KnowledgeGraphBuilder(
                neo4j_client=neo4j_client,
                llm_client=llm_client
            )
            
            logger.info("çŸ¥è¯†å›¾è°±æ„å»ºå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±æ„å»ºå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    return kg_builder


@celery_app.task(bind=True, name="backend.celery_tasks.process_text_chunk")
def process_text_chunk(
    self,
    chunk_text: str,
    chunk_id: str,
    file_id: str,
    task_id: str
    ) -> Dict[str, Any]:
    """
    å¤„ç†æ–‡æœ¬å?ä½¿ç”¨ LLM æå–å®ä½“å’Œå…³ç³?
    
    Args:
        chunk_text: æ–‡æœ¬å—å†…å®?
        chunk_id: æ–‡æœ¬å?ID
        file_id: æ–‡ä»¶ ID
        task_id: ä»»åŠ¡ ID
        
    Returns:
        å¤„ç†ç»“æœ
    """
    try:
        logger.info(f"å¼€å§‹å¤„ç†æ–‡æœ¬å—: {chunk_id} (ä»»åŠ¡: {task_id})")
        
        # è·å–çŸ¥è¯†å›¾è°±æ„å»ºå™?
        builder = get_kg_builder()
        
        # å¤„ç†æ–‡æœ¬å?
        result = builder.process_text(chunk_text)
        
        logger.info(
            f"æ–‡æœ¬å—å¤„ç†å®Œæˆ? {chunk_id}, "
            f"å®ä½“æ•?{result.get('entities_created', 0)}, "
            f"å…³ç³»æ•?{result.get('relations_created', 0)}"
        )
        
        return {
            "success": True,
            "chunk_id": chunk_id,
            "file_id": file_id,
            "task_id": task_id,
            "entities_created": result.get("entities_created", 0),
            "relations_created": result.get("relations_created", 0),
            "sentences_processed": result.get("sentences_processed", 0),
        }
        
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡æœ¬å—å¤±è´? {chunk_id}, é”™è¯¯: {e}")
        # é‡è¯•ä»»åŠ¡
        raise self.retry(exc=e, countdown=60, max_retries=3)


@celery_app.task(bind=True, name="backend.celery_tasks.build_knowledge_graph")
def build_knowledge_graph(
    self,
    file_id: str,
    task_id: str,
    chunks: list
) -> Dict[str, Any]:
    """
    æ„å»ºçŸ¥è¯†å›¾è°±(åè°ƒä»»åŠ¡)
    
    Args:
        file_id: æ–‡ä»¶ ID
        task_id: ä»»åŠ¡ ID
        chunks: æ–‡æœ¬å—åˆ—è¡?[{"chunk_id": str, "text": str}, ...]
        
    Returns:
        æ„å»ºç»“æœ
    """
    try:
        logger.info(f"å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°? {file_id} (ä»»åŠ¡: {task_id})")
        
        # æäº¤æ‰€æœ‰æ–‡æœ¬å—å¤„ç†ä»»åŠ¡
        tasks = []
        for chunk in chunks:
            chunk_id = chunk.get("chunk_id")
            chunk_text = chunk.get("text")
            
            if not chunk_id or not chunk_text:
                continue
            
            # æäº¤ä»»åŠ¡
            task = process_text_chunk.delay(
                chunk_text=chunk_text,
                chunk_id=chunk_id,
                file_id=file_id,
                task_id=task_id
            )
            tasks.append(task)
        
        logger.info(f"å·²æäº?{len(tasks)} ä¸ªæ–‡æœ¬å—å¤„ç†ä»»åŠ¡")
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ?å¯é€?æ ¹æ®éœ€æ±‚å†³å®šæ˜¯å¦ç­‰å¾?
        # results = [task.get() for task in tasks]
        
        return {
            "success": True,
            "file_id": file_id,
            "task_id": task_id,
            "chunks_count": len(chunks),
            "tasks_submitted": len(tasks),
        }
        
    except Exception as e:
        logger.error(f"æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {file_id}, é”™è¯¯: {e}")
        raise self.retry(exc=e, countdown=60, max_retries=3)


@celery_app.task(bind=True, name="backend.celery_tasks.download_and_process_chunks")
def download_and_process_chunks(
    self,
    hdfs_path: str,
    file_id: str,
    task_id: str
) -> Dict[str, Any]:
    """
    ä»?HDFS ä¸‹è½½æ–‡æœ¬å—å¹¶å¤„ç†
    
    Args:
        hdfs_path: HDFS ä¸­çš„æ–‡æœ¬å—ç›®å½•è·¯å¾?
        file_id: æ–‡ä»¶ ID
        task_id: ä»»åŠ¡ ID
        
    Returns:
        å¤„ç†ç»“æœ
    """
    try:
        logger.info(f"å¼€å§‹ä» HDFS ä¸‹è½½æ–‡æœ¬å? {hdfs_path}")
        
        # åˆ›å»º HDFS å®¢æˆ·ç«?
        hdfs_host = os.getenv("HADOOP_NAMENODE", "localhost:9000").split(":")[0]
        hdfs_port = int(os.getenv("HADOOP_NAMENODE", "localhost:9000").split(":")[1] if ":" in os.getenv("HADOOP_NAMENODE", "localhost:9000") else "9000")
        hdfs_client = HDFSClient(host=hdfs_host, port=hdfs_port)
        
        # åˆ—å‡ºæ–‡æœ¬å—æ–‡ä»?
        chunk_files = hdfs_client.list_files(hdfs_path)
        
        if not chunk_files:
            logger.warning(f"HDFS è·¯å¾„ä¸­æ²¡æœ‰æ–‡ä»? {hdfs_path}")
            return {
                "success": False,
                "error": "HDFS è·¯å¾„ä¸­æ²¡æœ‰æ–‡ä»?,
                "file_id": file_id,
                "task_id": task_id,
            }
        
        # ä¸‹è½½å¹¶å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
        chunks = []
        local_temp_dir = Path("/tmp/chunks")  # ä¸´æ—¶ç›®å½•
        local_temp_dir.mkdir(parents=True, exist_ok=True)
        
        for chunk_file in chunk_files:
            try:
                # ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ä¸´æ—¶ç›®å½?
                local_file = local_temp_dir / Path(chunk_file).name
                hdfs_client.download_file(chunk_file, str(local_file))
                
                # è¯»å–æ–‡æœ¬å†…å®¹
                with open(local_file, "r", encoding="utf-8") as f:
                    # æ–‡ä»¶æ ¼å¼:æ–‡ä»¶è·¯å¾„_å—ç¼–å?\t æ–‡æœ¬å—å†…å®?
                    line = f.read().strip()
                    if line:
                        parts = line.split('\t', 1)
                        if len(parts) >= 2:
                            chunk_key = parts[0]
                            chunk_text = parts[1]
                            chunks.append({
                                "chunk_id": chunk_key,
                                "text": chunk_text
                            })
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                local_file.unlink()
                
            except Exception as e:
                logger.error(f"ä¸‹è½½å’Œå¤„ç†æ–‡æœ¬å—å¤±è´¥: {chunk_file}, é”™è¯¯: {e}")
                continue
        
        logger.info(f"ä»?HDFS ä¸‹è½½äº?{len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        # è°ƒç”¨æ„å»ºçŸ¥è¯†å›¾è°±ä»»åŠ¡
        if chunks:
            result = build_knowledge_graph.delay(
                file_id=file_id,
                task_id=task_id,
                chunks=chunks
            )
            return {
                "success": True,
                "file_id": file_id,
                "task_id": task_id,
                "chunks_count": len(chunks),
                "build_task_id": result.id,
            }
        else:
            return {
                "success": False,
                "error": "æ²¡æœ‰å¯ç”¨çš„æ–‡æœ¬å—",
                "file_id": file_id,
                "task_id": task_id,
            }
        
    except Exception as e:
        logger.error(f"ä»?HDFS ä¸‹è½½æ–‡æœ¬å—å¤±è´? {hdfs_path}, é”™è¯¯: {e}")
        raise self.retry(exc=e, countdown=60, max_retries=3)


@celery_app.task(name="backend.celery_tasks.get_task_status")
def get_task_status(task_id: str) -> Dict[str, Any]:
    """
    è·å–ä»»åŠ¡çŠ¶æ€?
    
    Args:
        task_id: Celery ä»»åŠ¡ ID
        
    Returns:
        ä»»åŠ¡çŠ¶æ€ä¿¡æ?
    """
    try:
        from celery.result import AsyncResult
        
        result = AsyncResult(task_id, app=celery_app)
        
        return {
            "task_id": task_id,
            "status": result.status,
            "ready": result.ready(),
            "successful": result.successful() if result.ready() else None,
            "failed": result.failed() if result.ready() else None,
            "result": result.result if result.ready() else None,
            "traceback": result.traceback if result.failed() else None,
        }
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´? {task_id}, é”™è¯¯: {e}")
        return {
            "task_id": task_id,
            "status": "ERROR",
            "error": str(e),
        }



